![image](https://github.com/unexpected0/ControlNet/assets/92572887/f5ca271e-4fad-480a-adbf-9fda323a5071)![image](https://github.com/unexpected0/ControlNet/assets/92572887/c604944f-dd9c-4ce3-9567-f793917a770e)# ControlNet
基于ControlNet与Stable Diffusion的参考上色模型

在许多计算机视觉任务中，将黑白图像上色是一个具有挑战性但又非常有趣的问题。为了解决这个问题，我们借鉴了《Adding conditional control to text-to-image diffusion models》的思想，该工作通过引入条件控制来提高图像生成模型的灵活性和性能。我们希望能够利用参考彩色图像的信息，通过扩散模型为黑白图像添加真实感的颜色。

我们的训练数据来自于5部电影，分别为《叶问》《芳华》《黑豹2》《好莱坞圣诞》《安静的女孩》，将所有的帧提取出来，一共超过12万张照片，测试集采用《西线无战事》中的片段。于每一帧计算相邻n帧之间的相似度，当相似度在比较高范围内，例如0.9到0.99之间，保留图片对作为一组数据，调用当前帧时我们在所有的可行的数据组里随机抽取一组。

若数据组相似度过高，模型趋于将参考图直接输出，对于训练过程的帮助比较小，因此我们会设置相似度的上限和最小间隔帧数。实验过程中，相似度范围为0.94到0.97，最小间隔帧数为3帧。
采用Stable Diffusion v21作为模型原型，实验过程中使用5张48G的A6000进行实验，主干网络完全锁住，batch size选取15，采用ddp分布式加速方式，采用Adam优化器优化。

由于计算资源有限，三个版本在较小数据集（8000组数据）上，各训练100次，一个epoch花费12分钟左右，一共需要约18小时，由于训练时间非常短，实验数据的泛化能力比较差，对于数据集内部的数据进行测试，参考图采用第一帧，分别对第二三四帧上色，设置图片尺寸为512，DDIM步数为20，大致效果如下图所示。

![image](https://github.com/unexpected0/ControlNet/assets/92572887/09e0daa0-09d2-46ad-bd65-aadbec3e593b)

由上图可知，v2的整体色调和整体的视觉效果会明显好于v1和v3版本，更加接近参考图和Ground Truth，我们认为v1版本未在噪声中混入参考图编码特征，可能学习效率较低，而v3版本中将黑白图像扩展成3通道，再进行VQGAN编码，可能会对模型产生一定程度的误导，而且自编码器本身就没有对3通道黑白图像进行编解码，他的编码解码过程并不可逆，所以也会大大降低训练效率。因此，为了提高训练效率，在大样本进行训练的时候我们选择v2版本进行训练。

我们将v2版本在制作的5部电影的数据集上，训练成本实在太大，在5张A6000上以batch size为15需要约12小时每个epoch，我们训练了30个epoch，用时2周，模型在训练集和测试集上的效果分别如下图所示。

![image](https://github.com/unexpected0/ControlNet/assets/92572887/23d368da-f97f-487a-8e81-93fbf641ef96)

从上图的效果可以看出，我们的方法在完成不改变灰度图像内容的情况下的上色工作，图片各部分的内容信息没有被破坏，上色之后边缘没有偏移或者消失。训练集上的效果相对好一些，电影《芳华》（第四行）效果最好，《圣诞》（第三行）和《黑豹2》（第二行）的效果也比较好，边缘准确而且图片整体色差比较小，进一步训练应该会更好，但在电影《叶问4》上的效果明显比较差，图像质量会大幅下降，主要原因在于边界变得模糊和颜色的轻微偏移，但从下图可以看出，在测试集上的颜色的准确性有待提升，边缘清晰但色差很严重。

![image](https://github.com/unexpected0/ControlNet/assets/92572887/3f25f423-0103-404b-9571-d1de0bc45173)

我们认为我们模型表现不佳，可能的原因有以下三点：（1）自编码器会导致细致图片的细节丢失，例如比较小的人脸上色效果不好；（2）训练集数据的多样性不足，Stable Diffusion Model的训练集总共超过10T，而我们的仅仅100G左右

后续我们还探讨了不同的生成方式，我们假设一段黑白视频的第一帧的彩色图像是已知的，那我们的方法支持两种上色方式，一种是始终以第一帧作为参考帧，我们称为隔帧生成方式，也是我们之前在测试的时候用到的生成方式；另一种与之对应的声场方式是后续生成过程的参考帧都是上一次的生成结果，我们称之为递归生成，两种方法对比效果如下图所示。

![image](https://github.com/unexpected0/ControlNet/assets/92572887/ccbedc4e-ed9f-41b7-a9d8-428bf9915250)

图中前两列是参考图和第一帧的生成图像，对于两种生成模式是没有区别的，对于每一组图像上面一行对应隔帧生成方式，下面一行对应用递归生成方式。由上图我们可以清楚地看到，由于递归生成误差的传递和叠加，此种方式会明显增大颜色的偏差，在电影《黑豹2》（第二组图像）中体现尤为明显，第一帧只有极少部分的红色，第二帧第三帧大幅扩散。我们认为递归生成方式目前的效果明显弱于隔帧的生成效果。

本次实验测试了基于ControlNet框架完成参考图像上色任务，在内容保持和边缘重合度上表现比较好，但Stable Diffusion无论有多么神奇，他仅仅是一个深度模型，需要学习更多的数据才能表现出更好的效果，我们的模型由于训练不足和数据集多样性的缺陷，上色准确率还有待提升，测试集的迁移上色效果和泛化能力极其不稳定。

运行前请先将annotator.zip压缩包解压。

运行 python tutorial_train_sd21_color_v2.py训练模型。

